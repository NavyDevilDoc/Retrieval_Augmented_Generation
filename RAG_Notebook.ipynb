{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ensure you have the following installed. If not, install them using pip. Two things to note: \n",
    "- 'DocArrayInMemorySearch' is a method to use your local machine to store your documents if you don't have a Pinecone account.\n",
    "- PyPDFLoader is a method to load PDF files. If you don't have any PDF files, you can use TextLoader to load text files.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To build a .env file, you can start with a normal text file and rename it to .env. Make sure it's either \n",
    "in your working directory or you specify the path to the file. Load your OpenAI and Pinecone API keys into\n",
    "your .env file and call it here. I've seen tutorials and documentation that leave \"load_dotenv()\" empty, but\n",
    "for whatever reason, that did not work for me, so using Python, I specified the path to the .env file in \n",
    "between the quotation marks.\n",
    "'''\n",
    "\n",
    "load_dotenv(r\"\")\n",
    "OPENAI_API_KEY = os.getenv(\"\")\n",
    "PINECONE_API_KEY = os.getenv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Select your model and load the embeddings. You can choose from the following models:\n",
    "- GPT-3.5-turbo\n",
    "- Gemma 2B\n",
    "- Mixtral 8x7B\n",
    "- Llama2 7b\n",
    "'''\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "#MODEL = \"gemma\"\n",
    "#MODEL = \"mixtral\"\n",
    "#MODEL = \"llama2:7b\"\n",
    "\n",
    "# Build the model, parser, and prompt\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load your text, either in a text file or a PDF file. If you're using a PDF file, you can \n",
    "use the PyPDFLoader. Chunk size is the number of individual segments your text will be split \n",
    "into. Chunk overlap is the number of characters that will be repeated in each segment. Overlap \n",
    "is useful for ensuring that the model can understand the context of the text.\n",
    "'''\n",
    "\n",
    "# Load the text\n",
    "loader_txt = TextLoader(r\"\")\n",
    "text_documents = loader_txt.load()\n",
    "# Split the text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(text_documents)\n",
    "print(f\"Number of chunks: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parser: This is a string output parser, which is used to parse the output from the \n",
    "model as a normal text string instead of something far less readable.\n",
    "\n",
    "template: This is the template that will be used to generate the prompt. The context \n",
    "and question will be filled in with the actual question. Note that you don't need to\n",
    "give the model any context, as it will be provided in the template. One interesting\n",
    "aspect of the template is that you can modify your own template to fit your needs.\n",
    "\n",
    "prompt: This passes the template into the prompt chain. \n",
    "'''\n",
    "\n",
    "parser = StrOutputParser()\n",
    "template = \"\"\"\n",
    "Based on the context provided, answer the question \n",
    "with a detailed explanation. If the question is unclear or \n",
    "lacks sufficient context to provide an informed answer, \n",
    "respond with \"I don't know\" or ask for clarification.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please ensure your answer is thorough and detailed, offering \n",
    "insights and explanations to support your conclusions.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load your Pinecone API key and specify your index name. The index name, which is \n",
    "either added on the website or in the code below, should be unique to your project.\n",
    "'''\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pinecone is in the process of shifting to a fully serverless model. Since I'm on the east coast, \n",
    "I always default to AWS us-east-1. If you're on the west coast, you can change the region to \n",
    "the west region. The dimension is the number of dimensions in the vector space. The metric is\n",
    "the distance metric used to calculate the similarity between vectors. \n",
    "\n",
    "Let's talk about the dimension for a minute. While I'm not going to get into an in-depth discussion\n",
    "about it, all we're talking about is the vector's dimensionality. GPT's embeddings fluctuate based\n",
    "on which one you choose. I used \"text-embedding-ada-002\", which has a dimensionality of 1536. The\n",
    "open-source models noted previously all have dimensionality of 4096. A good thing about Pinecone is\n",
    "that it will tell you if your dimensionality is incorrect. For example, when I used Llama2 for the \n",
    "first time, I entered 5125 for the dimensionality, and Pinecone told me that it did not equal 4096,\n",
    "which was a pretty clear indicator that the proper dimensionality was 4096.\n",
    "'''\n",
    "\n",
    "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)\n",
    "pc.create_index(index_name, \n",
    "                dimension=4096, \n",
    "                metric='cosine', \n",
    "                spec=spec)\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I included this as a check to ensure that the index was properly created\n",
    "and that I wasn't loading duplicate documents into the index.\n",
    "'''\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Upload the documents to the Pinecone index. \n",
    "\n",
    "### Only run this once per document set because it will duplicate the documents \n",
    "in the index if run multiple times withoutloading a new set of documents. ###\n",
    "'''\n",
    "pinecone = PineconeVectorStore.from_documents(documents, \n",
    "                                              embeddings, \n",
    "                                              index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### Use this block to run the model on the Pinecone index without loading new documents. ###\n",
    "\n",
    "In a later update, I'll probably incorporate a conditional statement that combines the\n",
    "two PineconeVectorStore blocks into one. This will allow you to either load new documents\n",
    "or run the model on the existing documents in the index.\n",
    "'''\n",
    "\n",
    "pinecone = PineconeVectorStore.from_existing_index(index_name, \n",
    "                                                   embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Chain together the context, question, prompt, model, and parser. If you don't use a variable\n",
    "in conjunction with chain.invoke, the output will be printed to the console. \n",
    "'''\n",
    "\n",
    "chain = (\n",
    "    {\"context\": pinecone.as_retriever(), \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "stored_text = chain.invoke(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a nifty little helper function I wrote to format the text. In a \n",
    "future iteration, I'll save the output directly to a text file. \n",
    "'''\n",
    "\n",
    "def format_text(input_text, n=100):\n",
    "    # First pass: Add a newline after each colon\n",
    "    input_text = input_text.replace(':', ':\\n')\n",
    "    \n",
    "    # Second pass: Add a newline every n characters, taking the new lines into account\n",
    "    formatted_text = ''\n",
    "    current_length = 0  # Track the current length of the line\n",
    "    \n",
    "    for word in input_text.split(' '):  # Split the text into words\n",
    "        word_length = len(word)\n",
    "        if current_length + word_length > n:\n",
    "            # If adding the next word exceeds the limit, start a new line\n",
    "            formatted_text += '\\n' + word\n",
    "            current_length = word_length\n",
    "        else:\n",
    "            # Otherwise, add the word to the current line\n",
    "            if formatted_text:  # Add a space before the word if it's not the start of the text\n",
    "                formatted_text += ' '\n",
    "                current_length += 1  # Account for the added space\n",
    "            formatted_text += word\n",
    "            current_length += word_length\n",
    "        \n",
    "        # Account for newlines within the word itself (e.g., after a colon)\n",
    "        newline_count = word.count('\\n')\n",
    "        if newline_count > 0:\n",
    "            # Reset the current length for new lines\n",
    "            current_length = word_length - word.rfind('\\n') - 1\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "# Format the text with n=100\n",
    "formatted_text = format_text(stored_text, 100)\n",
    "\n",
    "# Print the formatted text\n",
    "print(formatted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
