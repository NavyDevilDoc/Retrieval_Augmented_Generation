{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ensure you have the following installed. If not, install them using pip. Two things to note: \n",
    "- 'DocArrayInMemorySearch' is a method to use your local machine to store your documents if you don't have a Pinecone account.\n",
    "- PyPDFLoader is a method to load PDF files. If you don't have any PDF files, you can use TextLoader to load text files.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.document_loaders.base import Document\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from operator import itemgetter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a nifty little helper function I wrote to format the text. In a \n",
    "future iteration, I'll save the output directly to a text file. \n",
    "'''\n",
    "\n",
    "def format_text(input_text, n=100):\n",
    "    # First pass: Add a newline after each colon\n",
    "    input_text = input_text.replace(':', ':\\n')\n",
    "    \n",
    "    # Second pass: Add a newline every n characters, taking the new lines into account\n",
    "    formatted_text = ''\n",
    "    current_length = 0  # Track the current length of the line\n",
    "    \n",
    "    for word in input_text.split(' '):  # Split the text into words\n",
    "        word_length = len(word)\n",
    "        if current_length + word_length > n:\n",
    "            # If adding the next word exceeds the limit, start a new line\n",
    "            formatted_text += '\\n' + word\n",
    "            current_length = word_length\n",
    "        else:\n",
    "            # Otherwise, add the word to the current line\n",
    "            if formatted_text:  # Add a space before the word if it's not the start of the text\n",
    "                formatted_text += ' '\n",
    "                current_length += 1  # Account for the added space\n",
    "            formatted_text += word\n",
    "            current_length += word_length\n",
    "        \n",
    "        # Account for newlines within the word itself (e.g., after a colon)\n",
    "        newline_count = word.count('\\n')\n",
    "        if newline_count > 0:\n",
    "            # Reset the current length for new lines\n",
    "            current_length = word_length - word.rfind('\\n') - 1\n",
    "    \n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get embeddings\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = transformer_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Function to perform semantic chunking\n",
    "def semantic_chunking(text, threshold=0.75):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "    initial_chunks = splitter.split_text(text)\n",
    "\n",
    "    if not initial_chunks:\n",
    "        print(\"No chunks generated for the text\")\n",
    "        return []\n",
    "    \n",
    "    chunk_embeddings = [get_embeddings(chunk) for chunk in initial_chunks]\n",
    "    \n",
    "    merged_chunks = []\n",
    "    current_chunk = initial_chunks[0]\n",
    "    current_embedding = chunk_embeddings[0]\n",
    "    \n",
    "    for i in range(1, len(initial_chunks)):\n",
    "        similarity = torch.cosine_similarity(\n",
    "            torch.tensor(current_embedding), torch.tensor(chunk_embeddings[i]), dim=0\n",
    "        ).item()\n",
    "        \n",
    "        if similarity > threshold:\n",
    "            current_chunk += \" \" + initial_chunks[i]\n",
    "            current_embedding = get_embeddings(current_chunk)\n",
    "        else:\n",
    "            merged_chunks.append(current_chunk)\n",
    "            current_chunk = initial_chunks[i]\n",
    "            current_embedding = chunk_embeddings[i]\n",
    "    \n",
    "    merged_chunks.append(current_chunk)\n",
    "    \n",
    "    return merged_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process documents in batches\n",
    "def batch_process_documents(documents, batch_size=10):\n",
    "    batched_documents = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        batched_documents.append(batch)\n",
    "    return batched_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To build a .env file, you can start with a normal text file and rename it to .env. Make sure it's either \n",
    "in your working directory or you specify the path to the file. Load your OpenAI and Pinecone API keys into\n",
    "your .env file and call it here. I've seen tutorials and documentation that leave \"load_dotenv()\" empty, but\n",
    "for whatever reason, that did not work for me, so using Python, I specified the path to the .env file in \n",
    "between the quotation marks.\n",
    "'''\n",
    "\n",
    "print(\"Loading environment variables...\")\n",
    "load_dotenv(r\"C:/Users/jspri/Desktop/AI_ML_Folder/Python_Practice_Folder/Natural_Language_Processing/EDQP_RAG_Model/env_variables.env\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Select your model and load the embeddings. You can choose from the following models:\n",
    "- GPT-4o\n",
    "- Llama3 70b\n",
    "'''\n",
    "\n",
    "#MODEL = \"gpt-4o\"\n",
    "MODEL = \"llama3:70b-instruct\"\n",
    "\n",
    "# Build the model, parser, and prompt\n",
    "print(f\"Loading model and embeddings for {MODEL}\")\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL, temperature=0.2, top_k=5, top_p=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load your text, either in a text file or a PDF file. If you're using a PDF file, you can \n",
    "use the PyPDFLoader. Chunk size is the number of individual segments your text will be split \n",
    "into. Chunk overlap is the number of characters that will be repeated in each segment. Overlap \n",
    "is useful for ensuring that the model can understand the context of the text.\n",
    "'''\n",
    "\n",
    "# Load transformer model and tokenizer for semantic chunking\n",
    "transformer_model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "transformer_model = AutoModel.from_pretrained(transformer_model_name)\n",
    "\n",
    "print(\"Loading text documents...\")\n",
    "loader = PyPDFLoader(r\"C:\\Users\\jspri\\Desktop\\AI_ML_Folder\\Python_Practice_Folder\\Natural_Language_Processing\\Source_Documents\\MANMED_CH_15.pdf\")\n",
    "text_documents = loader.load()\n",
    "\n",
    "print(\"Performing semantic chunking on text documents...\")\n",
    "documents = []\n",
    "for i, text_document in enumerate(text_documents):\n",
    "    print(f\"Chunking document {i + 1} of {len(text_documents)}...\")\n",
    "    # Extract the text content from the Document object\n",
    "    semantic_chunks = semantic_chunking(text_document.page_content)\n",
    "    if not semantic_chunks:\n",
    "        print(f\"No chunks generated for document {i + 1}\")\n",
    "    # Convert chunks back to Document objects\n",
    "    for chunk in semantic_chunks:\n",
    "        documents.append(Document(page_content=chunk))\n",
    "\n",
    "print(f\"Number of semantic chunks created: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load your Pinecone API key and specify your index name. The index name, which is \n",
    "either added on the website or in the code below, should be unique to your project.\n",
    "'''\n",
    "\n",
    "print(\"Setting up Pinecone...\")\n",
    "document_name = 'navmed-p117'\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "if 'llama' in MODEL:\n",
    "    model_name = 'llama3-70b-instruct'\n",
    "else:\n",
    "    model_name = MODEL\n",
    "index_name = f'{document_name}-{model_name}'\n",
    "print(f\"Pinecone index: '{index_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pinecone is in the process of shifting to a fully serverless model. Since I'm on the east coast, \n",
    "I always default to AWS us-east-1. If you're on the west coast, you can change the region to \n",
    "the west region. The dimension is the number of dimensions in the vector space. The metric is\n",
    "the distance metric used to calculate the similarity between vectors. \n",
    "\n",
    "Let's talk about the dimension for a minute. While I'm not going to get into an in-depth discussion\n",
    "about it, all we're talking about is the vector's dimensionality. GPT's embeddings fluctuate based\n",
    "on which one you choose. I used \"text-embedding-ada-002\", which has a dimensionality of 1536. The\n",
    "open-source models noted previously all have dimensionality of 4096. A good thing about Pinecone is\n",
    "that it will tell you if your dimensionality is incorrect. For example, when I used Llama2 for the \n",
    "first time, I entered 5125 for the dimensionality, and Pinecone told me that it did not equal 4096,\n",
    "which was a pretty clear indicator that the proper dimensionality was 4096.\n",
    "'''\n",
    "\n",
    "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "if index_name in pc.list_indexes().names():\n",
    "    print(f\"Deleting existing index: {index_name}\")\n",
    "    pc.delete_index(index_name)\n",
    "print(f\"Creating new index: {index_name}\")\n",
    "pc.create_index(index_name, \n",
    "                dimension=1536 if 'gpt' in MODEL else 4096,  \n",
    "                metric='cosine', \n",
    "                spec=spec)\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I included this as a check to ensure that the index was properly created\n",
    "and that I wasn't loading duplicate documents into the index.\n",
    "'''\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "print(f\"Index '{index_name}' ready for use\")\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parser: This is a string output parser, which is used to parse the output from the \n",
    "model as a normal text string instead of something far less readable.\n",
    "\n",
    "template: This is the template that will be used to generate the prompt. The context \n",
    "and question will be filled in with the actual question. Note that you don't need to\n",
    "give the model any context, as it will be provided in the template. One interesting\n",
    "aspect of the template is that you can modify your own template to fit your needs.\n",
    "\n",
    "prompt: This passes the template into the prompt chain. \n",
    "'''\n",
    "\n",
    "parser = StrOutputParser()\n",
    "template = \"\"\"\n",
    "Based on the context provided, answer the question \n",
    "with a detailed explanation. If the question is unclear or \n",
    "lacks sufficient context to provide an informed answer, \n",
    "respond with \"I don't know\" or ask for clarification. Spell\n",
    "out all acronyms. Provide references for major points \n",
    "based on the context provided. \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Only use information from the designated documentation.\n",
    "Ensure your answer is thorough and detailed, offering \n",
    "insights and explanations to support your conclusions.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Upload the documents to the Pinecone index. \n",
    "\n",
    "Set the 'data_storage' variable corresponding to the following:\n",
    "- 0: Upload documents to Pinecone\n",
    "- 1: Use an existing Pinecone index\n",
    "- 2: Use DocArrayInMemorySearch to store documents on your local machine\n",
    "'''\n",
    "\n",
    "# Print the active index and prompt the user for input\n",
    "print(f\"Active Index: {index_name}\")\n",
    "\n",
    "# Prompt the user to confirm the index name\n",
    "user_input = input(\"Enter 1 to continue using this index name or 2 to exit: \")\n",
    "\n",
    "# Check the user input\n",
    "if user_input == \"2\":\n",
    "    print(\"Exiting as requested by the user.\")\n",
    "    sys.exit(0)\n",
    "elif user_input != \"1\":\n",
    "    print(\"Invalid input. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "data_storage = 0\n",
    "if data_storage == 0:\n",
    "    print(f\"Uploading documents to Pinecone index {index_name}\")\n",
    "    datastore = PineconeVectorStore.from_documents(documents, \n",
    "                                                embedding=embeddings, \n",
    "                                                index_name=index_name)\n",
    "    print(\"Finished uploading documents to Pinecone index.\")\n",
    "\n",
    "elif data_storage == 1:\n",
    "    print(f\"Using existing Pinecone index {index_name} \")\n",
    "    datastore = PineconeVectorStore.from_existing_index(index_name, \n",
    "                                                   embeddings)\n",
    "    print(\"Finished pulling documents from Pinecone index.\")\n",
    "\n",
    "elif data_storage == 2:\n",
    "    print(\"Storing documents locally\")\n",
    "    datastore = DocArrayInMemorySearch.from_documents(documents, embeddings)\n",
    "    print(\"Finished uploading documents to local storage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Chain together the context, question, prompt, model, and parser. If you don't use a variable\n",
    "in conjunction with chain.invoke, the output will be printed to the console. \n",
    "'''\n",
    "\n",
    "retriever = datastore.as_retriever()\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "questions = [\"Please give a detailed summary of physical qualifications for submarine duty.\",\n",
    "             \"Please give a detailed summary of physical qualifications for special warfare duty.\",\n",
    "             \"Please give a detailed summary of the requirements for active duty separation and retirement.\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {format_text(chain.invoke({'question': question}),100)}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
