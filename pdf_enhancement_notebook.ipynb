{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "from PyPDF2 import PdfReader\n",
    "import pytesseract\n",
    "from pathlib import Path\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gc  # Garbage collection\n",
    "\n",
    "# Download necessary NLTK data\n",
    "'''\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I've been working on building out the text cleaning process because better text yields\n",
    "better results. Tokenizing words is something I'm not terribly familiar with, but\n",
    "filtering nonsense words are my specialty, especially with three teenagers at home\n",
    "who speak nothing but nonsense. \n",
    "'''\n",
    "\n",
    "def clean_text(text, return_tokens=True, filter_nonsense=True):\n",
    "    \"\"\"\n",
    "    Perform standard text pre-processing steps and optionally filter out nonsense words.\n",
    "    Parameters:\n",
    "    - text: Input text to clean.\n",
    "    - return_tokens: If True, return a list of tokens instead of a string.\n",
    "    - filter_nonsense: If True, remove words not found in a predefined valid words list.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Optionally filter out nonsense words\n",
    "    if filter_nonsense:\n",
    "        tokens = [word for word in tokens if word in valid_words]\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Return based on the return_tokens flag\n",
    "    return filtered_tokens if return_tokens else ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prior to running the PDF images through OCR, images are enhanced for improved OCR accuracy.\n",
    "'''\n",
    "\n",
    "def enhance_image(image):\n",
    "\n",
    "    try:\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
    "        # Apply Gaussian blur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "        # Apply thresholding to binarize the image\n",
    "        _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        # Detect and correct skew\n",
    "        coords = np.column_stack(np.where(thresh > 0))\n",
    "        angle = cv2.minAreaRect(coords)[-1]\n",
    "        if angle < -45:\n",
    "            angle = -(90 + angle)\n",
    "        else:\n",
    "            angle = -angle\n",
    "        (h, w) = thresh.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        rotated = cv2.warpAffine(thresh, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "        return rotated\n",
    "    except Exception as e:\n",
    "        print(f\"Error in image enhancement: {e}\")\n",
    "        return image  # Return original image if enhancement fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Little function to get the number of pages in the document.\n",
    "It isn't technically necessary at all, but it's nice to have.\n",
    "'''\n",
    "\n",
    "def get_num_pages(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        return len(pdf_reader.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert a multi-page PDF to images in chunks, enhance each image, perform OCR,\n",
    "clean the text, and write results to two text files: one with raw OCR output\n",
    "and another with cleaned text.\n",
    "\"\"\"\n",
    "\n",
    "def process_pdf_and_ocr(pdf_path, output_text_file, cleaned_output_file, chunk_size):\n",
    "    if not Path(pdf_path).is_file():\n",
    "        print(f\"Error: The file {pdf_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Determine the total number of pages\n",
    "    total_pages = get_num_pages(pdf_path)\n",
    "    print(f\"Total number of pages: {total_pages}\")\n",
    "    current_page = 1\n",
    "\n",
    "    while current_page <= total_pages:\n",
    "        last_page = min(current_page + chunk_size - 1, total_pages)\n",
    "        try:\n",
    "            images = convert_from_path(pdf_path, first_page=current_page, last_page=last_page)\n",
    "            with open(output_text_file, \"a\") as raw_file, open(cleaned_output_file, \"a\") as cleaned_file:\n",
    "                for image in images:\n",
    "                    print(f\"Processing page {current_page}...\")\n",
    "                    enhanced_image = enhance_image(image)\n",
    "                    text = pytesseract.image_to_string(enhanced_image)\n",
    "                    raw_file.write(text + \"\\n\")\n",
    "\n",
    "                    # Call clean_text with return_tokens=True and handle both list and string outputs\n",
    "                    cleaned_text = clean_text(text, return_tokens=True, filter_nonsense=True)\n",
    "                    \n",
    "                    # Check if cleaned_text is a list and join if necessary\n",
    "                    if isinstance(cleaned_text, list):\n",
    "                        cleaned_text = ' '.join(cleaned_text)\n",
    "                    \n",
    "                    cleaned_file.write(cleaned_text + \"\\n\")\n",
    "                    current_page += 1\n",
    "                    # Optional: Run garbage collection after each image is processed\n",
    "                    del image, enhanced_image, text, cleaned_text\n",
    "                    gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during OCR processing on pages {current_page}-{last_page}: {e}\")\n",
    "            current_page += chunk_size  # Skip the chunk and try the next one\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Send it.\n",
    "\n",
    "Make sure to update the pdf_path, output_text_file, and cleaned_output_file variables\n",
    "'''\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Reading PDF and performing OCR...\")\n",
    "\n",
    "valid_words = set(words.words())\n",
    "\n",
    "pdf_path = r\"\"\n",
    "output_text_file = r\"\"\n",
    "cleaned_output_file = r\"\"\n",
    "\n",
    "chunk_size = 50\n",
    "process_pdf_and_ocr(pdf_path, output_text_file, cleaned_output_file, chunk_size)\n",
    "print(f\"Processing completed. Raw OCR text written to {output_text_file}\")\n",
    "print(f\"Cleaned text written to {cleaned_output_file}\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Processing complete in {end_time - start_time:.2f} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Builder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
